{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Question_3_SQUAD.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqKCAR877O02"
      },
      "source": [
        "# This question was not completed.\n",
        "\n",
        "Sadly, I didn't have enough time to complete the third question. Nevertheless, I will write down my ideas of what I wanted to achieve after the provided cells.\n",
        "\n",
        "After loading in the tokenizer and the model, I thought of creating sentence embeddings for the SQUAD dataset, as well as adding the START and END tokens needed for the question-answering task.\n",
        "\n",
        "For the training of the model, my idea was to use the AdamW optimizer. For the fine tuning needed, I wanted to experiment with the learning rate, since this is arguably the most important parameter. Some more parameters for fine tuning could have been: the batch size, the max sequence length and the max query length.\n",
        "\n",
        "After training the model, I had the idea of plotting the Train vs Loss epoch and then loading in the dev-v2.0 dataset for the evaluation.\n",
        "\n",
        "After the evaluation, I had the idea of making one more cell in order for the user to enter his own input (one question and the passage with the answer) in order to get answers from the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWLMceVGFWMd"
      },
      "source": [
        "import torch\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "  print(\"Warning: No GPU found. Please add GPU to your notebook\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYWUnI5QSnFl"
      },
      "source": [
        "!pip install pytorch-pretrained-bert pytorch-nlp pytorch_transformers "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwodRQT3S56y",
        "outputId": "8f797e84-744c-4baf-9e04-306d6f5e47eb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g11E5zkWTZNL"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_transformers import BertTokenizer, BertConfig, BertModel\n",
        "from pytorch_transformers import AdamW, BertForQuestionAnswering\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "\n",
        "\n",
        "input_file = '/content/gdrive/My Drive/train-v2.0.json'\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 6,
      "outputs": []
    }
  ]
}