{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Question_5_GloVe.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOB9hhxJpi19ymyZeDDxsoY"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"0AvCbJ3zSiFB"},"source":["# Mounting content from Google Drive."]},{"cell_type":"code","metadata":{"id":"wP9xxI516Cnj","executionInfo":{"status":"ok","timestamp":1605517156948,"user_tz":-120,"elapsed":27901,"user":{"displayName":"PantMal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4GGhaxo8vt8nogujaxNMhd4zuU4fsatN5THk=s64","userId":"12150468585972673292"}},"outputId":"63242bf2-6276-4f95-da21-be13d4c72a84","colab":{"base_uri":"https://localhost:8080/"}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"L7caSXtdSkYk"},"source":["# Part 1: Loading and preprocessing the data"]},{"cell_type":"code","metadata":{"id":"103cAXPb6aAF","executionInfo":{"status":"ok","timestamp":1605517227214,"user_tz":-120,"elapsed":67924,"user":{"displayName":"PantMal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4GGhaxo8vt8nogujaxNMhd4zuU4fsatN5THk=s64","userId":"12150468585972673292"}},"outputId":"310deeee-6bc1-4e36-c951-33e917214823","colab":{"base_uri":"https://localhost:8080/"}},"source":["from sklearn.linear_model import LogisticRegression\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","import sys\n","import os\n","import re\n","from nltk.corpus import stopwords\n","import nltk\n","nltk.download('stopwords')\n","\n","tweet_path = \"/content/gdrive/My Drive/SentimentTweets.csv\"\n","\n","#Creating the dataframe and converting every uppercase character to lowercase using the str.lower() function.\n","tweet_df = pd.read_csv(tweet_path).apply(lambda x: x.astype(str).str.lower())\n","\n","#We will substitute every unwanted character with ' '. Here we remove the URLs.\n","tweet_df['text'] = tweet_df['text'].apply(lambda y: re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', \" \", y , flags=re.MULTILINE) )\n","\n","#Here we remove escape characters such as \\n, \\x and \\u.\n","tweet_df['text'] = tweet_df['text'].apply(lambda z: re.sub(r'\\\\n', \" \", z , flags=re.MULTILINE) )\n","tweet_df['text'] = tweet_df['text'].apply(lambda z: re.sub(r'\\\\x..', \" \", z , flags=re.MULTILINE) )\n","tweet_df['text'] = tweet_df['text'].apply(lambda z: re.sub(r'\\\\u....', \" \", z , flags=re.MULTILINE) )\n","#tweet_df['text'] = tweet_df['text'].apply(lambda z: re.sub(r'@\\w+', ' ', z , flags=re.MULTILINE) )\n","\n","#And finally we remove any other remaining symbols by removing every non-alphabetic character.\n","tweet_df['text'] = tweet_df['text'].apply(lambda k: re.sub(\"[^a-z]+\", \" \", k, flags=re.MULTILINE) )\n","\n","#Now we remove the stopwords.\n","stop = stopwords.words('english')\n","tweet_df['text'] = tweet_df['text'].apply(lambda s: ' '.join([item for item in s.split() if item not in stop]))\n","\n","#Converting the labels from 0-4 to Y-N.\n","tweet_df['target'] = tweet_df['target'].apply(lambda k: re.sub(\"4\", \"y\", k, flags=re.MULTILINE) )\n","tweet_df['target'] = tweet_df['target'].apply(lambda k: re.sub(\"0\", \"n\", k, flags=re.MULTILINE) )\n","\n","#Converting the labels to numeric format.\n","ltoi = {l: i for i, l in enumerate(tweet_df['target'].unique())}\n","tweet_df['target'] = tweet_df['target'].apply(lambda y: ltoi[y])\n","\n","#Since the dataset is too large, a subset of the data will be used for the GloVe model. (Otherwise the session crashes from too much RAM usage)\n","less_size = (int)(tweet_df.shape[0]/1.5)\n","tweet_df = tweet_df[:less_size]"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GeEtzbBfTNFw"},"source":["# Part 2: Stemming and lemmatization "]},{"cell_type":"code","metadata":{"id":"bKXu9nEdm0jV","executionInfo":{"status":"ok","timestamp":1605517250779,"user_tz":-120,"elapsed":759,"user":{"displayName":"PantMal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4GGhaxo8vt8nogujaxNMhd4zuU4fsatN5THk=s64","userId":"12150468585972673292"}},"outputId":"64704ebd-8556-47c4-e7a5-3c93405198af","colab":{"base_uri":"https://localhost:8080/"}},"source":["#Showing the 'text' column after the preprocessing.\n","tweet_df['text']"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0                                            brokenpromises\n","1         david carradine sad thai law sure fowl play ma...\n","2                                   b tell bro say congrats\n","3                                    littlefluffycat indeed\n","4         completed race life mins girlies work fun bloo...\n","                                ...                        \n","853328    think diversity deserved win susan boyle succe...\n","853329    starting collection sock yarn seems must knit ...\n","853330                            ryan comes back afternoon\n","853331    bundacp leave jakrta friday back monday night ...\n","853332                         hello app reviewers apple hq\n","Name: text, Length: 853333, dtype: object"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"eQb1nXygTTiA"},"source":["# Stemming"]},{"cell_type":"code","metadata":{"id":"bX6_8CIbm3w3","executionInfo":{"status":"ok","timestamp":1605517373689,"user_tz":-120,"elapsed":120556,"user":{"displayName":"PantMal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4GGhaxo8vt8nogujaxNMhd4zuU4fsatN5THk=s64","userId":"12150468585972673292"}},"outputId":"5a34775c-2208-4d5a-aa4b-15b2a58b6665","colab":{"base_uri":"https://localhost:8080/"}},"source":["from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","\n","import warnings\n","warnings.filterwarnings('ignore') \n","\n","import nltk\n","\n","#Objects needed for the stemming.\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","porter_stemmer = PorterStemmer()\n","\n","#Defining a stem_sentences function that will stem our text and return it in string format.\n","def stem_sentences(sentence):\n","    tokens = sentence.split()\n","    stemmed_tokens = [porter_stemmer.stem(token) for token in tokens]\n","    return ' '.join(stemmed_tokens)\n","\n","#Stemming the text.\n","tweet_df['stemmed_text'] = tweet_df['text'].apply(stem_sentences)\n","\n","#Showing the 'text' column after stemming.\n","tweet_df['stemmed_text']"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0                                              brokenpromis\n","1         david carradin sad thai law sure fowl play man...\n","2                                    b tell bro say congrat\n","3                                      littlefluffycat inde\n","4         complet race life min girli work fun bloodi ho...\n","                                ...                        \n","853328    think divers deserv win susan boyl success reg...\n","853329    start collect sock yarn seem must knit often h...\n","853330                             ryan come back afternoon\n","853331    bundacp leav jakrta friday back monday night w...\n","853332                             hello app review appl hq\n","Name: stemmed_text, Length: 853333, dtype: object"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"6SQL2pdZTVOv"},"source":["# Lemmatization"]},{"cell_type":"code","metadata":{"id":"eCv1CD_sm6k4","executionInfo":{"status":"ok","timestamp":1605517410181,"user_tz":-120,"elapsed":33197,"user":{"displayName":"PantMal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4GGhaxo8vt8nogujaxNMhd4zuU4fsatN5THk=s64","userId":"12150468585972673292"}},"outputId":"c0f3154d-1a33-4059-e57c-b1bba60362b8","colab":{"base_uri":"https://localhost:8080/"}},"source":["nltk.download('wordnet')\n","\n","#Objects needed for the lemmatization.\n","tokenizer = nltk.tokenize.WhitespaceTokenizer()\n","lemmatizer = nltk.stem.WordNetLemmatizer()\n","\n","#Defining a lemmatize function that will lemmatize our text and return it in string format.\n","def lemmatize(text):\n","    string_list = [lemmatizer.lemmatize(word) for word in tokenizer.tokenize(text)]\n","    list_to_str = ' '.join([str(element) for element in string_list])\n","    return list_to_str\n","\n","#Performing lemmatization on the stemmed text.\n","tweet_df['lemmatized_text'] = tweet_df['stemmed_text'].apply(lemmatize)\n","\n","#Showing the lemmatized text.\n","tweet_df['lemmatized_text']"],"execution_count":5,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["0                                              brokenpromis\n","1         david carradin sad thai law sure fowl play man...\n","2                                    b tell bro say congrat\n","3                                      littlefluffycat inde\n","4         complet race life min girli work fun bloodi ho...\n","                                ...                        \n","853328    think diver deserv win susan boyl success rega...\n","853329    start collect sock yarn seem must knit often h...\n","853330                             ryan come back afternoon\n","853331    bundacp leav jakrta friday back monday night w...\n","853332                             hello app review appl hq\n","Name: lemmatized_text, Length: 853333, dtype: object"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"i9PDwhYPTfe2"},"source":["# Part 3: Preparing the data using TorchText"]},{"cell_type":"markdown","metadata":{"id":"QHQ5b90WTmKO"},"source":["# Initializing the data Fields"]},{"cell_type":"code","metadata":{"id":"KSRNtkQQXgUP","executionInfo":{"status":"ok","timestamp":1605517420411,"user_tz":-120,"elapsed":5752,"user":{"displayName":"PantMal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4GGhaxo8vt8nogujaxNMhd4zuU4fsatN5THk=s64","userId":"12150468585972673292"}}},"source":["import torch\n","from torchtext import data\n","\n","#Contrary to the TF-IDF model, torchtext was used in order to prepare the data for the model.\n","\n","#Here we initialize two fields. One for the text (our features) and one for the labels.\n","text_field = data.Field(tokenize='spacy', lower=True, fix_length=40) #Fix length is 40 so every batch can have the same length (needed for this model).\n","label_field = data.Field(sequential=False, use_vocab=False)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JJ8Le1IuTrwz"},"source":["# Splitting the dataframe and getting csv format"]},{"cell_type":"code","metadata":{"id":"1r7HQcYxYn02","executionInfo":{"status":"ok","timestamp":1605517428511,"user_tz":-120,"elapsed":6499,"user":{"displayName":"PantMal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4GGhaxo8vt8nogujaxNMhd4zuU4fsatN5THk=s64","userId":"12150468585972673292"}}},"source":["#Splitting the dataframe to train and test sets.\n","train_set, test_set = train_test_split(tweet_df, test_size=0.2, random_state=42)\n","\n","#Keeping the columns we need.\n","train_set = train_set[['lemmatized_text','target']]\n","test_set = test_set[['lemmatized_text','target']]\n","\n","#Creating two new csv files (needed for the following function).\n","train_set.to_csv('/content/gdrive/My Drive/train_file.csv')\n","test_set.to_csv('/content/gdrive/My Drive/test_file.csv')"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QJ3VB2V6T10G"},"source":["# Splitting again with TorchText"]},{"cell_type":"code","metadata":{"id":"C2oaQzy_X4Iw","executionInfo":{"status":"ok","timestamp":1605517565434,"user_tz":-120,"elapsed":134709,"user":{"displayName":"PantMal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4GGhaxo8vt8nogujaxNMhd4zuU4fsatN5THk=s64","userId":"12150468585972673292"}},"outputId":"2b42a7a0-db85-4155-f2bf-0255af94ca12","colab":{"base_uri":"https://localhost:8080/"}},"source":["#Splitting again with the TabularDataset module. This will allow us to also tokenize the text.\n","train_data, test_data = data.TabularDataset.splits(\n","                                        path='/content/gdrive/My Drive/',\n","                                        train = 'train_file.csv',\n","                                        test = 'train_file.csv',\n","                                        format = 'csv',\n","                                        skip_header = True,\n","                                        fields=[(None, None), ('lemmatized_text', text_field), ('target', label_field)])\n","\n","#Printing the first entry to make sure the tokenization was succesful.\n","print(vars(train_data.examples[0]))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["{'lemmatized_text': ['can', 'not', 'upload', 'profil', 'pic', 'whywhi'], 'target': '0'}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"r13xJSzTT_qe"},"source":["# Building the GloVe vocabulary"]},{"cell_type":"code","metadata":{"id":"Gs9gTykmcf_x","executionInfo":{"status":"ok","timestamp":1605518435224,"user_tz":-120,"elapsed":783398,"user":{"displayName":"PantMal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4GGhaxo8vt8nogujaxNMhd4zuU4fsatN5THk=s64","userId":"12150468585972673292"}},"outputId":"d7763c62-9974-4ce8-a2b8-5bfe8456a8bd","colab":{"base_uri":"https://localhost:8080/"}},"source":["#We'll have to use a fixed size for the the vocabulary.\n","#The vocab size without any restrictions is too large, and as a result it makes the training take way too much time. \n","#On the 'bright' side, those 25000 words are the most frequent from the text.\n","MAX_VOCAB_SIZE = 25_000\n","\n","#Building the GloVe vocabulary (using a Twitter embedding for obvious reasons...)\n","text_field.build_vocab(\n","    train_data,\n","    max_size = MAX_VOCAB_SIZE,\n","    vectors='glove.twitter.27B.25d'\n",")\n","\n","#Getting the vocab instance.\n","vocab = text_field.vocab"],"execution_count":9,"outputs":[{"output_type":"stream","text":[".vector_cache/glove.twitter.27B.zip: 1.52GB [11:43, 2.16MB/s]                           \n","100%|█████████▉| 1193268/1193514 [00:28<00:00, 43846.28it/s]"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"Cbv8DOIkUDE8"},"source":["# Creating batches"]},{"cell_type":"code","metadata":{"id":"fzOMM4JjfxTd","executionInfo":{"status":"ok","timestamp":1605518443187,"user_tz":-120,"elapsed":433,"user":{"displayName":"PantMal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4GGhaxo8vt8nogujaxNMhd4zuU4fsatN5THk=s64","userId":"12150468585972673292"}},"outputId":"d5c158a3-ff7f-42d0-ea4e-7c42a5142aa4","colab":{"base_uri":"https://localhost:8080/"}},"source":["#Since the dataset is rather large we'll have to use an equally large size for the batches.\n","BATCH_SIZE = 1000\n","\n","#Splitting with BucketIterator (which will also give us the iterator in tensor format).\n","train_iterator, test_iterator = data.BucketIterator.splits(\n","    (train_data, test_data), \n","    batch_size = BATCH_SIZE,\n","    sort=False)\n","\n","#Checking how many batches we have.\n","len(test_iterator)"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["683"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"vyFi0ZKGUKGR"},"source":["# Part 4: Creating the Model"]},{"cell_type":"code","metadata":{"id":"vJR_1jqTf_4e","executionInfo":{"status":"ok","timestamp":1605518514259,"user_tz":-120,"elapsed":791,"user":{"displayName":"PantMal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4GGhaxo8vt8nogujaxNMhd4zuU4fsatN5THk=s64","userId":"12150468585972673292"}}},"source":["import torch.nn as nn\n","import torchvision.transforms as transforms\n","import torchvision.datasets as dsets\n","import torch.nn.functional as F\n","\n","#Defining the Feed Forward GloVe Model. Commentary about the choices of the dimensions, activation functions etc can be seen in the ReadMe file.\n","class FeedForwardGloVeModel(nn.Module):\n","    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n","        super().__init__()\n","\n","        #Embedding layer\n","        self.embedding = nn.Embedding(input_dim,embedding_dim)\n","\n","        #Linear layer\n","        self.fc = nn.Linear(hidden_dim*embedding_dim,output_dim)\n","        \n","    def forward(self, input):\n","        \n","        #Embedding layer (also resized for the output function).\n","        embed = self.embedding(input).view(input.size()[0], -1)    \n","        \n","        #Non-linearity.\n","        act = F.relu(embed)\n","\n","        #Linear layer\n","        output = self.fc(embed)\n","\n","        return output\n","\n","\n","INPUT_DIM = len(text_field.vocab) #Input size is the length of the GloVe vocab.\n","EMBEDDING_DIM = 300 #'Random' number for the embedding dimensions.\n","HIDDEN_DIM = 40 #Hidden dimension must be equal to the batch length for this model.\n","OUTPUT_DIM = 2\n","\n","#Initializing the model.\n","model = FeedForwardGloVeModel(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cORTtHRXUSKU"},"source":["# Part 5: Training the Model and getting scores"]},{"cell_type":"code","metadata":{"id":"w4LP9pRqyvtT","executionInfo":{"status":"ok","timestamp":1605519580675,"user_tz":-120,"elapsed":1057236,"user":{"displayName":"PantMal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4GGhaxo8vt8nogujaxNMhd4zuU4fsatN5THk=s64","userId":"12150468585972673292"}},"outputId":"f5966d1b-759e-4d3e-dccd-27324ad671ba","colab":{"base_uri":"https://localhost:8080/"}},"source":["import torch\n","from torchtext import data\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n","train_losses = []\n","test_losses = []\n","test_accuracies = []\n","prec_scores = []\n","rec_scores = []\n","f1_scores = []\n","\n","#Initializing the Loss Function and the Optimizer.\n","loss_function = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adamax(model.parameters(), lr=0.02)\n","\n","epochs = 5\n","for epoch in range(epochs): #Training the model for 5 epochs. \n","    for batch in train_iterator: #Epochs are way less than the ones in TF-IDF, but in this case we'll have to iterate every batch which needs more time.\n","      \n","        optimizer.zero_grad()\n","        prediction = model(batch.lemmatized_text.T)\n","        loss = loss_function(prediction, batch.target)\n","        \n","        loss.backward()\n","        optimizer.step()\n","        \n","        train_losses.append(loss.item())\n","    \n","    for batch in test_iterator:\n","        with torch.no_grad(): #Turn off gradients for test validation, saves memory and computations.\n","            optimizer.zero_grad()\n","            prediction = model(batch.lemmatized_text.T)\n","            loss = loss_function(prediction, batch.target)\n","            \n","            test_losses.append(loss.item())\n","\n","            ps = torch.exp(prediction)\n","            top_p, top_class = ps.topk(1, dim=1)\n","            equals = top_class == batch.target.view(*top_class.shape)\n","            \n","            #Getting precision, recall and f1 scores.\n","            prec_score_test = precision_score(top_class, batch.target, average = 'weighted')\n","            prec_scores.append(prec_score_test)\n","            \n","            rec_score_test = recall_score(top_class, batch.target, average = 'weighted')\n","            rec_scores.append(rec_score_test)\n","            \n","            f1_score_test = f1_score(top_class, batch.target, average = 'weighted')\n","            f1_scores.append(f1_score_test)\n","\n","            test_accuracy = torch.mean(equals.float())\n","            test_accuracies.append(test_accuracy)\n","    \n","    print(f\"Epoch: {epoch+1}/{epochs}.. \",\n","          f\"Training Loss: {np.mean(train_losses):.3f}.. \",\n","          f\"Test Loss: {np.mean(test_losses):.3f}.. \",\n","          f\"Test Accuracy: {np.mean(test_accuracies):.3f}\")\n","\n","prec_score_test = np.mean(prec_scores)\n","rec_score_test = np.mean(rec_scores)\n","f1_score_test = np.mean(f1_scores)\n","\n","#More discussion about the scores and comparisons with the other models can be seen in the ReadMe file.\n","print()\n","print(\"Printing scores for Precision, Recall and F1-Measure\")\n","print(\"Average accuracy using Precision : {}%\".format(round(prec_score_test*100,2)))\n","print(\"Average accuracy using Recall : {}%\".format(round(rec_score_test*100,2)))\n","print(\"Average accuracy using F1-Measure: {}%\".format(round(f1_score_test*100,2)))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Epoch: 1/5..  Training Loss: 2.580..  Test Loss: 0.897..  Test Accuracy: 0.705\n","Epoch: 2/5..  Training Loss: 1.779..  Test Loss: 0.796..  Test Accuracy: 0.721\n","Epoch: 3/5..  Training Loss: 1.427..  Test Loss: 0.763..  Test Accuracy: 0.713\n","Epoch: 4/5..  Training Loss: 1.230..  Test Loss: 0.714..  Test Accuracy: 0.723\n","Epoch: 5/5..  Training Loss: 1.102..  Test Loss: 0.683..  Test Accuracy: 0.732\n","\n","Printing scores for Precision, Recall and F1-Measure\n","Average accuracy using Precision : 77.58%\n","Average accuracy using Recall : 73.16%\n","Average accuracy using F1-Measure: 73.83%\n"],"name":"stdout"}]}]}