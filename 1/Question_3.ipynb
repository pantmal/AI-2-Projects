{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Question_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q38EntYTyNsK"
      },
      "source": [
        "# Mounting content from Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ps6Qx3IYnuh",
        "outputId": "25f4580b-2f9f-4ecb-eac8-1a70e4304f27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzeA8fO2yUhS"
      },
      "source": [
        "# Part 1: Loading and preprocessing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "048m0CInIaTL"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sys\n",
        "import os\n",
        "import re\n",
        "\n",
        "tweet_path = \"/content/gdrive/My Drive/SentimentTweets.csv\"\n",
        "\n",
        "#Creating the dataframe and converting every uppercase character to lowercase using the str.lower() function.\n",
        "tweet_df = pd.read_csv(tweet_path).apply(lambda x: x.astype(str).str.lower())\n",
        "\n",
        "#We will substitute every unwanted character with ' '. Here we remove the URLs.\n",
        "tweet_df['text'] = tweet_df['text'].apply(lambda y: re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', \" \", y , flags=re.MULTILINE) )\n",
        "\n",
        "#Here we remove escape characters such as \\n, \\x and \\u.\n",
        "tweet_df['text'] = tweet_df['text'].apply(lambda z: re.sub(r'\\\\n', \" \", z , flags=re.MULTILINE) )\n",
        "tweet_df['text'] = tweet_df['text'].apply(lambda z: re.sub(r'\\\\x..', \" \", z , flags=re.MULTILINE) )\n",
        "tweet_df['text'] = tweet_df['text'].apply(lambda z: re.sub(r'\\\\u....', \" \", z , flags=re.MULTILINE) )\n",
        "\n",
        "#And finally we remove any other remaining symbols by removing every non-alphabetic character.\n",
        "tweet_df['text'] = tweet_df['text'].apply(lambda k: re.sub(\"[^a-z]+\", \" \", k, flags=re.MULTILINE) )\n",
        "\n",
        "#Splitting the dataframe into train and test subsets.\n",
        "train_set, test_set = train_test_split(tweet_df, test_size=0.2, random_state=42)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e98E8AwFybTI"
      },
      "source": [
        "# Part 2: Stemming, lemmatization and initializing the TF-IDF array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMWut0STkkyt",
        "outputId": "4bb0ea67-be67-401e-c57e-b74a9bb56e3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "#Showing the 'text' column of the training set after the preprocessing.\n",
        "train_set['text']"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "263132      lalavazquez happy mother s day have a wonderf...\n",
              "615448     i m really going to miss jonasbrothers th june...\n",
              "158240     wonders kenapa ya terngiang lagu you never giv...\n",
              "1218246     beyondbirthday no problem can you do tha same...\n",
              "414653      billyscallywag you re welcome love the spider...\n",
              "                                 ...                        \n",
              "110268      zoeox iii can t breathe easy legends y ooh th...\n",
              "259178                      grienke with another loss today \n",
              "131932     so full of energy today spent the morning sing...\n",
              "671155      is about to watch the making of radioactive dvd \n",
              "121958     demolished some peanut butter honey toast and ...\n",
              "Name: text, Length: 1024000, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kyo24w8t_4u"
      },
      "source": [
        "# Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfPU_39qkYot",
        "outputId": "c1c5ff7b-4d09-408b-8ffb-c853d1e4f0e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore') \n",
        "\n",
        "import nltk\n",
        "\n",
        "\n",
        "#Objects needed for the stemming.\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "porter_stemmer = PorterStemmer()\n",
        "\n",
        "#Defining a stem_sentences function that will stem our text and return it in string format.\n",
        "def stem_sentences(sentence):\n",
        "    tokens = sentence.split()\n",
        "    stemmed_tokens = [porter_stemmer.stem(token) for token in tokens]\n",
        "    return ' '.join(stemmed_tokens)\n",
        "\n",
        "#Stemming the text.\n",
        "train_set['stemmed_text'] = train_set['text'].apply(stem_sentences)\n",
        "test_set['stemmed_text'] = test_set['text'].apply(stem_sentences)\n",
        "\n",
        "#Showing the 'text' column of the training set after stemming.\n",
        "train_set['stemmed_text']"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "263132     lalavazquez happi mother s day have a wonder a...\n",
              "615448     i m realli go to miss jonasbroth th june i wan...\n",
              "158240     wonder kenapa ya terngiang lagu you never give...\n",
              "1218246     beyondbirthday no problem can you do tha same me\n",
              "414653     billyscallywag you re welcom love the spiderma...\n",
              "                                 ...                        \n",
              "110268     zoeox iii can t breath easi legend y ooh thank...\n",
              "259178                          grienk with anoth loss today\n",
              "131932     so full of energi today spent the morn sing al...\n",
              "671155            is about to watch the make of radioact dvd\n",
              "121958     demolish some peanut butter honey toast and is...\n",
              "Name: stemmed_text, Length: 1024000, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diso5TLxuFKG"
      },
      "source": [
        "# Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1FDlgkhmOqL",
        "outputId": "c57d0e4f-57ab-4b73-a6f6-9f4e7f21b965",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "\n",
        "#Objects needed for the lemmatization.\n",
        "tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "#Defining a lemmatize function that will lemmatize our text and return it in string format.\n",
        "def lemmatize(text):\n",
        "    string_list = [lemmatizer.lemmatize(word) for word in tokenizer.tokenize(text)]\n",
        "    list_to_str = ' '.join([str(element) for element in string_list])\n",
        "    return list_to_str\n",
        "\n",
        "#Performing lemmatization on the stemmed text.\n",
        "train_set['lemmatized_text'] = train_set['stemmed_text'].apply(lemmatize)\n",
        "test_set['lemmatized_text'] = test_set['stemmed_text'].apply(lemmatize)\n",
        "\n",
        "#Showing the lemmatized text of the training set.\n",
        "train_set['lemmatized_text']"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "263132     lalavazquez happi mother s day have a wonder a...\n",
              "615448     i m realli go to miss jonasbroth th june i wan...\n",
              "158240     wonder kenapa ya terngiang lagu you never give...\n",
              "1218246     beyondbirthday no problem can you do tha same me\n",
              "414653     billyscallywag you re welcom love the spiderma...\n",
              "                                 ...                        \n",
              "110268     zoeox iii can t breath easi legend y ooh thank...\n",
              "259178                          grienk with anoth loss today\n",
              "131932     so full of energi today spent the morn sing al...\n",
              "671155            is about to watch the make of radioact dvd\n",
              "121958     demolish some peanut butter honey toast and is...\n",
              "Name: lemmatized_text, Length: 1024000, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7CzajgDuINS"
      },
      "source": [
        "# Initializing the TF-IDF array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Igk04gwXnfSy"
      },
      "source": [
        "#Initializing TF-IDF array with unigrams and bigrams.\n",
        "tfidf = TfidfVectorizer(ngram_range=(1,2))\n",
        "tfs_train = tfidf.fit_transform(train_set['lemmatized_text'])\n",
        "tfs_test = tfidf.transform(test_set['lemmatized_text'])\n",
        "\n",
        "#Getting the Y's.\n",
        "Y_train = train_set['target']\n",
        "Y_test = test_set['target']"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vh-QkuBLykzg"
      },
      "source": [
        "# Part 3: Classification and scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjI1G8Vuppvw",
        "outputId": "bdf8b5fb-10f0-4c5d-c990-db5cc02d70f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "#Performing the classification. max_iter argument was increased because the default one gave congression errors.\n",
        "reg_model = LogisticRegression(max_iter=500)\n",
        "reg_model.fit(tfs_train, Y_train)\n",
        "y_test_pred = reg_model.predict(tfs_test)\n",
        "\n",
        "#The classification report will give us the scores for precision, recall and F1 metrics.\n",
        "print(classification_report(Y_test,y_test_pred))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.82      0.82    128091\n",
            "           4       0.82      0.82      0.82    127909\n",
            "\n",
            "    accuracy                           0.82    256000\n",
            "   macro avg       0.82      0.82      0.82    256000\n",
            "weighted avg       0.82      0.82      0.82    256000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zqk49ik8ysCC"
      },
      "source": [
        "# Part 4: Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTxnokn0kW-W",
        "outputId": "d759e11b-ac7b-4930-d006-cf74f8db9c4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "#Finally we also perform a cross validation on the data.\n",
        "cross_val_score(reg_model,tfs_train, Y_train, cv = 10, n_jobs=4)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.81893555, 0.81916992, 0.81870117, 0.81824219, 0.81874023,\n",
              "       0.81855469, 0.81731445, 0.81958008, 0.81974609, 0.81847656])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    }
  ]
}